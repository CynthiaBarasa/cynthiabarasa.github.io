{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cynthycynthy/titanic-dataset-analysis?scriptVersionId=115364265\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Background of the Competition**\nThe Titanic dataset is a well-known dataset that provides information on the passengers who were onboard the fateful voyage of the RMS Titanic. The data includes details such as the passenger's name, age, gender, ticket class, fare paid, and information on their family members. The dataset also includes a column called \"Survived\" which indicates whether a passenger survived the disaster or not.\n\nThere are a total of 891 rows in the dataset, with 12 columns. Some of the key columns in the dataset include:\n\n• PassengerId: a unique identifier for each passenger\n• Survived: a binary variable that indicates whether the passenger survived (1) or did not survive (0) the disaster\n• Pclass: the ticket class of the passenger (1 = first class, 2 = second class, 3 = third class)\n• Name: the name of the passenger\n• Sex: the gender of the passenger (male or female)\n• Age: the age of the passenger (some values are missing)\n• SibSp: the number of siblings or spouses the passenger had on board\n• Parch: the number of parents or children the passenger had on board\n• Ticket: the ticket number of the passenger\n• Fare: the fare paid by the passenger\n• Cabin: the cabin number of the passenger (some values are missing)\n• Embarked: the port at which the passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)\n\nOverall, the key challenges I encountered when working on the Titanic dataset were: how to handle missing values and imbalanced classes, encode categorical variables, reduce the dimensionality of the dataset, and identify and handle noise in the data.\n\nHere are a few tips and resources that I found helpful when getting started in the Titanic dataset competition:\n\nGet familiar with the dataset\nPre-process the data\nSplit the data into training and test sets\nTry out a few different algorithms\nTune the hyper parameters\nEvaluate the model\nHere are a few resources that I found helpful as I started Working on the competition:\n• Kaggle's Titanic tutorial\n• scikit-learn documentation.\n• Pandas documentation","metadata":{}},{"cell_type":"code","source":" import pandas as pd   #data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-19T12:44:43.006601Z","iopub.execute_input":"2022-12-19T12:44:43.007047Z","iopub.status.idle":"2022-12-19T12:44:43.029866Z","shell.execute_reply.started":"2022-12-19T12:44:43.006958Z","shell.execute_reply":"2022-12-19T12:44:43.028945Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:09:10.879547Z","iopub.execute_input":"2022-12-19T13:09:10.879974Z","iopub.status.idle":"2022-12-19T13:09:10.89019Z","shell.execute_reply.started":"2022-12-19T13:09:10.879936Z","shell.execute_reply":"2022-12-19T13:09:10.888568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/titanic/train.csv') #Load data that was downloaded from Kaggle, train data\ntest = pd.read_csv('/kaggle/input/titanic/test.csv') #Load data that was downloaded from Kaggle, test data\ntest_ids = test[\"PassengerId\"]\n\ndef clean(data): #Created a clean function to get some data\n    data = data.drop([\"Ticket\", \"PassengerId\", \"Name\", \"Cabin\"], axis=1)\n    \n    #Dropped the Ticket, PassengerId, Name and Cabin because I think it doesn't give me a lot of information\n    \n    cols = [\"SibSp\", \"Parch\", \"Fare\", \"Age\"] #Columns that don't have a number in them\n    for col in cols: #going through the columns\n        data[col].fillna(data[col].median(), inplace=True) \n        #Converting the columns to numbers, fill in the numbers that are not filled with their mean.\n        \n    data.Embarked.fillna(\"U\", inplace=True) #Fill the embarked with missing datapoints with unknown tokens\n    return data\n\ndata = clean(data)\ntest = clean(test)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:10:37.143812Z","iopub.execute_input":"2022-12-19T13:10:37.144203Z","iopub.status.idle":"2022-12-19T13:10:37.190715Z","shell.execute_reply.started":"2022-12-19T13:10:37.144155Z","shell.execute_reply":"2022-12-19T13:10:37.189822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5) #Drop some columns and limit them to 5","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:10:50.567667Z","iopub.execute_input":"2022-12-19T13:10:50.56811Z","iopub.status.idle":"2022-12-19T13:10:50.590219Z","shell.execute_reply.started":"2022-12-19T13:10:50.568072Z","shell.execute_reply":"2022-12-19T13:10:50.589029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing #using sklearn to convert strings to actual values\nle = preprocessing.LabelEncoder() #Using the label encoder\ncolumns = [\"Sex\", \"Embarked\"]\n\nfor col in columns:\n    data[col] = le.fit_transform(data[col]) #Doing the mapping of the data column\n    test[col] = le.transform(test[col]) #Doing the mapping of the data column\n    print(le.classes_) # print to see the conversion of the classes to integer e.g Femle is 1\n      \ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:11:04.48888Z","iopub.execute_input":"2022-12-19T13:11:04.489306Z","iopub.status.idle":"2022-12-19T13:11:05.003455Z","shell.execute_reply.started":"2022-12-19T13:11:04.489269Z","shell.execute_reply":"2022-12-19T13:11:05.002564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using logistic regression to have a validation set to see how good it is\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\n\ny = data[\"Survived\"]\nX = data.drop(\"Survived\", axis=1) #Dropping the column for the survived\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:11:16.870859Z","iopub.execute_input":"2022-12-19T13:11:16.871243Z","iopub.status.idle":"2022-12-19T13:11:17.001668Z","shell.execute_reply.started":"2022-12-19T13:11:16.871212Z","shell.execute_reply":"2022-12-19T13:11:17.000115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train) #Logistic Regression to the classifier to specify the random state","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:11:28.025906Z","iopub.execute_input":"2022-12-19T13:11:28.026841Z","iopub.status.idle":"2022-12-19T13:11:28.063069Z","shell.execute_reply.started":"2022-12-19T13:11:28.026798Z","shell.execute_reply":"2022-12-19T13:11:28.062126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = clf.predict(X_val) #Know how good it is on the validation data that it hasn't seen\nfrom sklearn.metrics import accuracy_score #Getting the accuracy\naccuracy_score(y_val, predictions)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:11:39.494483Z","iopub.execute_input":"2022-12-19T13:11:39.494894Z","iopub.status.idle":"2022-12-19T13:11:39.506679Z","shell.execute_reply.started":"2022-12-19T13:11:39.494861Z","shell.execute_reply":"2022-12-19T13:11:39.505128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_preds = clf.predict(test) #Getting the submission Predictions","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:11:50.805857Z","iopub.execute_input":"2022-12-19T13:11:50.806828Z","iopub.status.idle":"2022-12-19T13:11:50.81396Z","shell.execute_reply.started":"2022-12-19T13:11:50.80679Z","shell.execute_reply":"2022-12-19T13:11:50.812486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generating a CSV file that can be submitted to Kaggle\ndf = pd.DataFrame({\"PassengerId\": test_ids.values,\n                   \"Survived\": submission_preds,\n                  })","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:12:01.496215Z","iopub.execute_input":"2022-12-19T13:12:01.496594Z","iopub.status.idle":"2022-12-19T13:12:01.501977Z","shell.execute_reply.started":"2022-12-19T13:12:01.496562Z","shell.execute_reply":"2022-12-19T13:12:01.501126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"final submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-19T13:12:14.229787Z","iopub.execute_input":"2022-12-19T13:12:14.230496Z","iopub.status.idle":"2022-12-19T13:12:14.240344Z","shell.execute_reply.started":"2022-12-19T13:12:14.230456Z","shell.execute_reply":"2022-12-19T13:12:14.239043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}